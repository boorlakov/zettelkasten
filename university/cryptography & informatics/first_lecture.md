# Лекция №1

[<- Назад (Основные понятия теории информации)](https://github.com/boorlakov/zettelkasten/blob/main/university/cryptography%20%26%20informatics/README.md)

## Введение

### Про теорию информации

Теория информации - прикладная математики + радиотехника + информатика про измерение информации, про предельные свойства соотношения для систем передачи данных

Использует математический аппарат как теорию вероятности и математическую статистику

Основые понятия:

- Энтропия информационная

- Количество информации

- Криптография

Основная статья, положившая основу дисциплины

[Статья "Математическая теория связи" (1948)](http://shannon.usu.edu.ru/Shannon/shannon1948/)

### Виды информации

- Дискретная точные значения величины (удобнее для человека)

- Непрерывная (датчики и т.д.)

Информация - нематериальная сущность, при помощи которой с любой точностью описывается любой понятийный факт

Сжать проще неравномерную информацию (и кстати она более криптостойкая(?))

`a = b` несет столько же инфы что и `a^3 = b^3`

мат ожидание + дисперсия содержит всю инфу о случайной величине

`Y = X + Z`

`X` это отправляемая инфа,
`Z` помехи

чем меньше дисперсия зэт тем больше инфы получим о икс

Про энтропию как рассеивание информации. Чем меньше энтропия, тем больше мы имеет инфы об объекте. на этом факте основывается метод исследования информации с помощью нейронок (grad down E -> лучше результат)

пусть есть алфавит A = {x1,...,xn}

X = (x1 -> P(x1),...,xn -> P(xn))

sum(X) = 1

Собственная информация = `log_b(1/P(xi))`, `b = 2` <=> `bit`

1. I(x)>=0, x from X

2. монотонность
3. аддитивность (у независимых событий) (т.к. инфа это логарифм, а логарифм произв это сумма логарифмов)

Взаимная инфа
`I(xj, yk) = log(P(xj | yk)/(P(xj) P(yk))`

Свойства:

-00 < I(xj, yk) < +00

Как связана колво инфы с энтропией

для дискретных веиличин икс игрик заданных законами распределения P(X==Xi, Y==Yj) = pj

I(X, Y) = sum_ij(log_2(pij)/(pi pj))

Определим энтропию через среднюю взаимную инфу как

I(X, Y) = sum i j pi log2(pij/pi qj)

I(X, X) = HX = HX = sum_i(pi*log2(1/pi)) = - sum_i(pi log2(pi))

I(X,Y) = I(Y,X)

I(X,Y) = HX + HY - H(X,Y)

H(X,Y) = - sum_ij(pij log2(pij))

I(X,Y) <=I(X,X)

HX = 0, X is const

1) I(X, Y) >= 0
e^x-1 >= x
x-1 >= lnx
